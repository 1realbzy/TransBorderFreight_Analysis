{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3398709 rows from 36 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hbempong\\AppData\\Local\\Temp\\ipykernel_22792\\1393031099.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, \"FREIGHT_DENSITY\"] = df[\"VALUE\"] / df[\"SHIPWT\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to C:\\Users\\hbempong\\TransBorderFreight_Analysis\\output\\merged_data_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# scripts/consolidate_data.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import time\n",
    "\n",
    "DATA_DIR = r\"C:\\Users\\hbempong\\TransBorderFreight_Analysis\\data\\2020\"\n",
    "OUTPUT_FILE = r\"C:\\Users\\hbempong\\TransBorderFreight_Analysis\\output\\merged_data_v2.csv\"\n",
    "\n",
    "# Mapping dictionary for human-readable labels\n",
    "MAPPINGS = {\n",
    "    \"DISAGMOT\": {\n",
    "        5: \"Truck\", 6: \"Rail\", 7: \"Pipeline\", 8: \"Air\", \n",
    "        9: \"Vessel\", 10: \"Other\", \"Unknown\": \"Unknown\"\n",
    "    },\n",
    "    \"CANPROV\": {\n",
    "        \"XA\": \"Alberta\", \"XB\": \"British Columbia\", \"XC\": \"Manitoba\", \n",
    "        \"XD\": \"New Brunswick\", \"XE\": \"Newfoundland and Labrador\", \n",
    "        \"XF\": \"Nova Scotia\", \"XG\": \"Ontario\", \"XH\": \"Prince Edward Island\",\n",
    "        \"XI\": \"Quebec\", \"XJ\": \"Saskatchewan\", \"XO\": \"Other\",\n",
    "        \"XX\": \"Unknown\"\n",
    "    },\n",
    "    \"MEXSTATE\": {\n",
    "        \"XX\": \"Unknown\", \"XO\": \"Other\", \"XM\": \"Mexico City\", \n",
    "        \"XQ\": \"Querétaro\", \"XY\": \"Yucatán\", \"XA\": \"Aguascalientes\",\n",
    "        \"XB\": \"Baja California\", \"XC\": \"Campeche\", \"XD\": \"Chiapas\",\n",
    "        \"XE\": \"Chihuahua\", \"XF\": \"Coahuila\", \"XG\": \"Colima\",\n",
    "        \"XH\": \"Durango\", \"XI\": \"Guanajuato\", \"XJ\": \"Guerrero\",\n",
    "        \"XK\": \"Hidalgo\", \"XL\": \"Jalisco\", \"XN\": \"Michoacán\",\n",
    "        \"XP\": \"Nayarit\", \"XR\": \"Oaxaca\", \"XS\": \"Puebla\",\n",
    "        \"XT\": \"Querétaro\", \"XU\": \"Quintana Roo\", \"XV\": \"San Luis Potosí\",\n",
    "        \"XW\": \"Sinaloa\", \"XZ\": \"Tamaulipas\", \"YA\": \"Tlaxcala\", \n",
    "        \"YB\": \"Veracruz\", \"YC\": \"Yucatán\", \"YD\": \"Zacatecas\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_and_merge_data(data_dir):\n",
    "    \"\"\"Load and merge all monthly CSV files into one DataFrame.\"\"\"\n",
    "    all_files = glob.glob(os.path.join(data_dir, \"*/*.csv\"))\n",
    "\n",
    "    dtype_spec = {\n",
    "        \"DISAGMOT\": \"Int64\",\n",
    "        \"CANPROV\": \"string\",\n",
    "        \"VALUE\": \"float64\",\n",
    "        \"SHIPWT\": \"float64\",\n",
    "        \"DEPE\": \"string\"\n",
    "    }\n",
    "\n",
    "    df_list = []\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            # Process files in chunks to save memory\n",
    "            chunk_iter = pd.read_csv(file, dtype=dtype_spec, low_memory=False, chunksize=100000)\n",
    "            for chunk in chunk_iter:\n",
    "                df_list.append(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"Loaded {len(df)} rows from {len(all_files)} files.\")\n",
    "    return df\n",
    "\n",
    "def infer_missing_values(df):\n",
    "    \"\"\"Infer missing values for CANPROV and MEXSTATE where possible.\"\"\"\n",
    "    if \"COUNTRY\" not in df.columns:\n",
    "        raise ValueError(\"Column 'COUNTRY' not found in the dataset.\")\n",
    "\n",
    "    # Fill missing CANPROV based on COUNTRY\n",
    "    df.loc[(df[\"CANPROV\"].isna()) & (df[\"COUNTRY\"] == 1220), \"CANPROV\"] = \"XO\"  # Assign \"Other\" for Canada\n",
    "    \n",
    "    # Fill missing MEXSTATE based on COUNTRY\n",
    "    df.loc[(df[\"MEXSTATE\"].isna()) & (df[\"COUNTRY\"] == 2010), \"MEXSTATE\"] = \"XO\"  # Assign \"Other\" for Mexico\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Clean and preprocess the dataset.\"\"\"\n",
    "    df = infer_missing_values(df)\n",
    "\n",
    "    # Replace codes with human-readable labels\n",
    "    for column, mapping in MAPPINGS.items():\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].map(mapping).fillna(\"Unknown\")\n",
    "\n",
    "    # Log missing values for review\n",
    "    for column in MAPPINGS.keys():\n",
    "        if column in df.columns:\n",
    "            missing_values = df[~df[column].isin(MAPPINGS[column].values())][column].unique()\n",
    "            if len(missing_values) > 0:\n",
    "                print(f\"Warning: Found unmapped values in {column}: {missing_values}\")\n",
    "\n",
    "    # Fix SHIPWT = 0 cases\n",
    "    df.loc[(df[\"SHIPWT\"] == 0) & (df[\"VALUE\"] > 0), \"SHIPWT\"] = df[\"SHIPWT\"].replace(0, df[\"SHIPWT\"].median())\n",
    "\n",
    "    # Drop rows where both SHIPWT and VALUE are 0\n",
    "    df = df[(df[\"SHIPWT\"] > 0) & (df[\"VALUE\"] > 0)]\n",
    "\n",
    "    # Feature Engineering: Calculate Freight Density\n",
    "    df.loc[:, \"FREIGHT_DENSITY\"] = df[\"VALUE\"] / df[\"SHIPWT\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_with_retry(df, output_file, retries=3, delay=5):\n",
    "    \"\"\"Save DataFrame to CSV with retry mechanism.\"\"\"\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f\"Cleaned data saved to {output_file}\")\n",
    "            return\n",
    "        except PermissionError:\n",
    "            print(f\"Attempt {i + 1} failed. Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "    print(\"Failed to save the file after multiple attempts. Please check file permissions.\")\n",
    "\n",
    "def main():\n",
    "    df = load_and_merge_data(DATA_DIR)\n",
    "    df_cleaned = clean_data(df)\n",
    "    \n",
    "    # Save the cleaned dataset\n",
    "    save_with_retry(df_cleaned, OUTPUT_FILE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
